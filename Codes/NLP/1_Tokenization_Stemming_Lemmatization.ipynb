{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1c745e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3e1ae54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to /home/admin1/nltk_data...\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to /home/admin1/nltk_data...\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
      "[nltk_data]    | Downloading package pe08 to /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
      "[nltk_data]    | Downloading package pil to /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
      "[nltk_data]    | Downloading package ptb to /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data]    | Downloading package qc to /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    | Downloading package rslp to /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
      "[nltk_data]    | Downloading package rte to /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to /home/admin1/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "554e692c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b85d85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"Like a flower in the dessert, I have to grow in the cruelest weather. Holding on to every drop of rain, just to stay alive. But it's not enough to survive, I want to bloom beneath the blazing sun. And show y'all the colours that live inside of me. I want you to see what I can become. `Christy Ann Martine`\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f7d62dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Like', 'a', 'flower', 'in', 'the', 'dessert,', 'I', 'have', 'to', 'grow', 'in', 'the', 'cruelest', 'weather.', 'Holding', 'on', 'to', 'every', 'drop', 'of', 'rain,', 'just', 'to', 'stay', 'alive.', 'But', \"it's\", 'not', 'enough', 'to', 'survive,', 'I', 'want', 'to', 'bloom', 'beneath', 'the', 'blazing', 'sun.', 'And', 'show', \"y'all\", 'the', 'colours', 'that', 'live', 'inside', 'of', 'me.', 'I', 'want', 'you', 'to', 'see', 'what', 'I', 'can', 'become.', '`Christy', 'Ann', 'Martine`']\n",
      "61\n"
     ]
    }
   ],
   "source": [
    "print(WhitespaceTokenizer().tokenize(data))\n",
    "print(len(WhitespaceTokenizer().tokenize(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "583a2c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.NLTK Word Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3497fba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a. Word and sentence tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98aea6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "text = \"Hope, is the only thing stronger than fear! #Hope #Amal.M\"\n",
    "text=\" $ student should having the id card around the neck while in the college campus*%#ðŸ¤—\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc332141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Like', 'a', 'flower', 'in', 'the', 'dessert', ',', 'I', 'have', 'to', 'grow', 'in', 'the', 'cruelest', 'weather', '.', 'Holding', 'on', 'to', 'every', 'drop', 'of', 'rain', ',', 'just', 'to', 'stay', 'alive', '.', 'But', 'it', \"'s\", 'not', 'enough', 'to', 'survive', ',', 'I', 'want', 'to', 'bloom', 'beneath', 'the', 'blazing', 'sun', '.', 'And', 'show', \"y'all\", 'the', 'colours', 'that', 'live', 'inside', 'of', 'me', '.', 'I', 'want', 'you', 'to', 'see', 'what', 'I', 'can', 'become', '.', '`', 'Christy', 'Ann', 'Martine', '`']\n",
      "72\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(data))\n",
    "print(len(word_tokenize(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e708b7d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Like a flower in the dessert, I have to grow in the cruelest weather.',\n",
       " 'Holding on to every drop of rain, just to stay alive.',\n",
       " \"But it's not enough to survive, I want to bloom beneath the blazing sun.\",\n",
       " \"And show y'all the colours that live inside of me.\",\n",
       " 'I want you to see what I can become.',\n",
       " '`Christy Ann Martine`']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11443b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#b. Punctuation Based Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b54fcb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Like', 'a', 'flower', 'in', 'the', 'dessert', ',', 'I', 'have', 'to', 'grow', 'in', 'the', 'cruelest', 'weather', '.', 'Holding', 'on', 'to', 'every', 'drop', 'of', 'rain', ',', 'just', 'to', 'stay', 'alive', '.', 'But', 'it', \"'\", 's', 'not', 'enough', 'to', 'survive', ',', 'I', 'want', 'to', 'bloom', 'beneath', 'the', 'blazing', 'sun', '.', 'And', 'show', 'y', \"'\", 'all', 'the', 'colours', 'that', 'live', 'inside', 'of', 'me', '.', 'I', 'want', 'you', 'to', 'see', 'what', 'I', 'can', 'become', '.', '`', 'Christy', 'Ann', 'Martine', '`']\n",
      "75\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "print(wordpunct_tokenize(data))\n",
    "print(len(wordpunct_tokenize(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b1a67d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#c. Treebank Word Tokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d75a68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Like', 'a', 'flower', 'in', 'the', 'dessert', ',', 'I', 'have', 'to', 'grow', 'in', 'the', 'cruelest', 'weather.', 'Holding', 'on', 'to', 'every', 'drop', 'of', 'rain', ',', 'just', 'to', 'stay', 'alive.', 'But', 'it', \"'s\", 'not', 'enough', 'to', 'survive', ',', 'I', 'want', 'to', 'bloom', 'beneath', 'the', 'blazing', 'sun.', 'And', 'show', \"y'all\", 'the', 'colours', 'that', 'live', 'inside', 'of', 'me.', 'I', 'want', 'you', 'to', 'see', 'what', 'I', 'can', 'become.', '`Christy', 'Ann', 'Martine`']\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "print(TreebankWordTokenizer().tokenize(data))\n",
    "print(len(TreebankWordTokenizer().tokenize(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a2ec70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#d. Tweet Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "675cf25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2efcce46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Like', 'a', 'flower', 'in', 'the', 'dessert', ',', 'I', 'have', 'to', 'grow', 'in', 'the', 'cruelest', 'weather', '.', 'Holding', 'on', 'to', 'every', 'drop', 'of', 'rain', ',', 'just', 'to', 'stay', 'alive', '.', 'But', \"it's\", 'not', 'enough', 'to', 'survive', ',', 'I', 'want', 'to', 'bloom', 'beneath', 'the', 'blazing', 'sun', '.', 'And', 'show', \"y'all\", 'the', 'colours', 'that', 'live', 'inside', 'of', 'me', '.', 'I', 'want', 'you', 'to', 'see', 'what', 'I', 'can', 'become', '.', '`', 'Christy', 'Ann', 'Martine', '`']\n",
      "71\n"
     ]
    }
   ],
   "source": [
    "print(TweetTokenizer().tokenize(data))\n",
    "print(len(TweetTokenizer().tokenize(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f65d84be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import MWETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7b48a322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['L', 'i', 'k', 'e', ' ', 'a', ' ', 'f', 'l', 'o', 'w', 'e', 'r', ' ', 'i', 'n', ' ', 't', 'h', 'e', ' ', 'd', 'e', 's', 's', 'e', 'r', 't', ',', ' ', 'I', ' ', 'h', 'a', 'v', 'e', ' ', 't', 'o', ' ', 'g', 'r', 'o', 'w', ' ', 'i', 'n', ' ', 't', 'h', 'e', ' ', 'c', 'r', 'u', 'e', 'l', 'e', 's', 't', ' ', 'w', 'e', 'a', 't', 'h', 'e', 'r', '.', ' ', 'H', 'o', 'l', 'd', 'i', 'n', 'g', ' ', 'o', 'n', ' ', 't', 'o', ' ', 'e', 'v', 'e', 'r', 'y', ' ', 'd', 'r', 'o', 'p', ' ', 'o', 'f', ' ', 'r', 'a', 'i', 'n', ',', ' ', 'j', 'u', 's', 't', ' ', 't', 'o', ' ', 's', 't', 'a', 'y', ' ', 'a', 'l', 'i', 'v', 'e', '.', ' ', 'B', 'u', 't', ' ', 'i', 't', \"'\", 's', ' ', 'n', 'o', 't', ' ', 'e', 'n', 'o', 'u', 'g', 'h', ' ', 't', 'o', ' ', 's', 'u', 'r', 'v', 'i', 'v', 'e', ',', ' ', 'I', ' ', 'w', 'a', 'n', 't', ' ', 't', 'o', ' ', 'b', 'l', 'o', 'o', 'm', ' ', 'b', 'e', 'n', 'e', 'a', 't', 'h', ' ', 't', 'h', 'e', ' ', 'b', 'l', 'a', 'z', 'i', 'n', 'g', ' ', 's', 'u', 'n', '.', ' ', 'A', 'n', 'd', ' ', 's', 'h', 'o', 'w', ' ', 'y', \"'\", 'a', 'l', 'l', ' ', 't', 'h', 'e', ' ', 'c', 'o', 'l', 'o', 'u', 'r', 's', ' ', 't', 'h', 'a', 't', ' ', 'l', 'i', 'v', 'e', ' ', 'i', 'n', 's', 'i', 'd', 'e', ' ', 'o', 'f', ' ', 'm', 'e', '.', ' ', 'I', ' ', 'w', 'a', 'n', 't', ' ', 'y', 'o', 'u', ' ', 't', 'o', ' ', 's', 'e', 'e', ' ', 'w', 'h', 'a', 't', ' ', 'I', ' ', 'c', 'a', 'n', ' ', 'b', 'e', 'c', 'o', 'm', 'e', '.', ' ', '`', 'C', 'h', 'r', 'i', 's', 't', 'y', ' ', 'A', 'n', 'n', ' ', 'M', 'a', 'r', 't', 'i', 'n', 'e', '`']\n",
      "306\n"
     ]
    }
   ],
   "source": [
    "print(MWETokenizer().tokenize(data))\n",
    "print(len(MWETokenizer().tokenize(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c788ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "02832015",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "db91484c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "28c17e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "08f6cb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"program\",\"programs\",\"programing\",\"programer\",\"programers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f8d160ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "program  :  program\n",
      "programs  :  program\n",
      "programing  :  program\n",
      "programer  :  program\n",
      "programers  :  program\n"
     ]
    }
   ],
   "source": [
    "for w in words:\n",
    "    print(w, \" : \", ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9fbb3698",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Programmers program with programming languages\"\n",
    "tokens = word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "72fa83e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Programmers  :  programm\n",
      "program  :  program\n",
      "with  :  with\n",
      "programming  :  program\n",
      "languages  :  languag\n"
     ]
    }
   ],
   "source": [
    "for w in tokens:\n",
    "    print(w, \" : \", ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bc8fb41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "713319b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d6864b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "snow_stemmer = SnowballStemmer(language = \"english\")\n",
    "words = ['cared', 'university', 'fairly', 'easily', 'melancholy', 'sings', 'sung', 'singer', 'sportingly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2706748b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cared------>care\n",
      "university------>univers\n",
      "fairly------>fair\n",
      "easily------>easili\n",
      "melancholy------>melancholi\n",
      "sings------>sing\n",
      "sung------>sung\n",
      "singer------>singer\n",
      "sportingly------>sport\n"
     ]
    }
   ],
   "source": [
    "stem_words = []\n",
    "for w in words:\n",
    "    x = snow_stemmer.stem(w)\n",
    "    stem_words.append(x)\n",
    "for w1, w2 in zip(words, stem_words):\n",
    "    print(w1 + \"------>\" + w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9218aec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "words = [\"carefully\",\"doing\",\"winner\",\"flighted\",\"dances\",\"easily\",\"doable\", \"manageable\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem_words = []\n",
    "for w in words:\n",
    "    x = lemmatizer.lemmatize(w)\n",
    "    lem_words.append(x)\n",
    "for w1, w2 in zip(words, lem_words):\n",
    "    print(w1 + \"------->\" + w2)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f82e9eef39fec3b4432bd3d531b07136b7f172564442eec42315ae171445fe6f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
